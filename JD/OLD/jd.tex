\documentclass[a4paper, 11pt]{article}

%\input{/Users/boissonn/Science/Papers/macros}

\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{url}

\setlength{\textwidth}{18cm}
\setlength{\evensidemargin}{-1cm}
\setlength{\oddsidemargin}{-1cm}

\newcommand{\ind}{\mbox{}\hspace{2cm}}
\newcommand{\sind}{\mbox{}\hspace{1cm}}
   
\parindent 0pt
\parskip 1mm

\newcommand{\R}{\mathbb R}

\begin{document}

\title{Algorithmic Foundations of \\%Geometric and Topological Data Analysis\\
Geometric Modeling in Higher Dimensions\\ (GeHDi)}
\author{Jean-Daniel Boissonnat}

\maketitle

\begin{abstract}
% Computational geometry has been very successful in providing efficient algorithms and codes for applications in 2 and 3 dimensions.  However, geometric data are ubiquitous and arise from many sources other than measurements of coordinates of points in our familiar 3D world. 

During the past decade, exceptional progress was made with Geometric Modeling and 3D data processing. The new field of Geometry Processing emerged, theoretical foundations together with very efficient practical solutions were obtained for ubiquitous/emblematic problems like surface meshing and surface reconstruction. Extending these techniques to higher dimensions would have tremendous applications in science and engineering but is currently extremely limited and asks for new algorithmic  breakthrough. This project aims at settling the algorithmic foundations of Geometric Modeling in higher dimensions and to propose a well-principled ground-breaking software platform allowing technological advances for varied applications in science and engineering.

\end{abstract}

\paragraph{TODO.} Expand the abstract. Add references. Bag of features. Challenges : préciser les questions ouvertes.

\paragraph{Questions.} 
Geometric Modeling sounds outdated. List of WP. 
\newpage

\input{pi}
\newpage

\input{synopsis}

\newpage

\input{proposal}

\end{document}
\newpage

\appendix

Geometric data are ubiquitous. They arise from measurements and simulations in the natural sciences, and from images, shapes, and text that can easily be converted into geometric point cloud data. Most of these data reside in high-dimensional spaces. For example, configuration spaces of robots with many degrees of freedom, or configuration spaces of macromolecules are subspaces of high-dimensional real vector spaces. Natural and artificial systems like biological or sensor networks are often described by a large number of real parameters, whereas a collection of text documents can be represented as a set of term frequency vectors in Euclidean space; similar interpretations can be given for image, audio, and video data. Hence, processing and analyzing geometric data in high-dimensional spaces is a core task in science and engineering.

At the heart of our proposed project is the insight that most of these data are structured, although this intrinsic geometric structure is often not easy to capture. Our long-term vision is to have efficient and reliable methods for geometric data analysis that find and exploit hidden structure and by that lead to fast and robust geometric data processing in high dimensions. In this project, we aim at laying the foundations of a new field—computational geometric learning— that provides efficient and reliable methods for geometric data analysis.

In the past, an important focus of computational geometry was the design and analysis of exact low-dimensional geometric algorithms and data structures. This focus is well reflected in one of the major success stories of computational geometry, namely the Computational Geom- etry Algorithms Library (CGAL) that provides easy access to efficient and reliable geometric algorithms in the form of a C++ library. In most cases, reliability is achieved through provable correctness guarantees.

Our proposed project aims at extending this success story—combining efficiency with cor- rectness guarantees—to high-dimensional geometric computation. This is not a straightforward task. For many problems, no efficient algorithms exist that compute the exact solution in high dimensions. Hence, there is a need for fast approximate solutions, but we do not want to sacrifice guarantees. The following two kinds of approximation guarantees are particularly desirable: first, the solution approximates an objective better if more time and memory re- sources are employed (algorithmic guarantee), and second, the approximation gets better when the data become more dense and/or more accurate (learning theoretic guarantee). Efficient and practical algorithms and data structures with such guarantees are indispensable for building robust software in the spirit of our long-term vision. With respect to this vision, the targeted breakthroughs of our project are

(i) the design and analysis of efficient and reliable algorithms and data structures for relevant high-dimensional data processing tasks; we aim at theoretical guarantees and efficiency, by exploiting intrinsic structure of the data;

(ii) the implementation of algorithms and data structures for high-dimensional geometric data processing tasks within CGAL, and

(iii) fast and robust application software (building on the fundamental algorithms and data structures) for specific important problems in computational biology, robotics and shape analysis.

=====================

(Maggioni) Our research will focus on three related aspects, each of which is important and of independent interest: Key issues: definition of local similarities, dimensionality reduction, parametrizations of the data, stability with respect to perturbation of the data (e.g. measurement noise, instrument normalization etc...). Properties of functions on the data.





% Carlsson: For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. 
In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. 

Carlsson : This project will develop topological tools for understanding qualitative properties of data sets. We will use homology as applied to data sets directly and to derived complexes to define invariants or signatures that distinguish between the underlying geometric objects. Important goals will include the identification, location, and classification of qualitative features of the data set, such as the presence of corners, edges, cone points, etc. and the use of homology applied to canonically defined blowups and tangent complexes to distinguish between two dimensional shapes in three dimensional Euclidean space. We will use the recently developed techniques of persistence and landmarking to make homology a stable and readily computable invariant. We will also develop the theory of multidimensional persistence, in which one studies spaces that are equipped with several parameters, in order to better understand data sets in which there are several different parameters describing different geometric properties of the space. The overall goal is to continue to develop and improve the available tools for studying qualitative information about geometric objects. The goal of this project is to develop tools for understanding data sets that are not easy to understand using standard methods of statistics and analysis. This kind of data might include singular points, or might be strongly curved. The data is also high dimensional, in the sense that each data point has many coordinates. For instance, we might have a data set whose points each of which is an image, which has one coordinate for each pixel. Many standard tools rely on linear approximations, which do not work well in strongly curved or singular problems. The kind of tools we have in mind are in part topological, in the sense that they measure more qualitative properties of the spaces involved, such as connectedness, or the number of holes in a space, and so on. For example, the project takes the point of view that it is better to understand qualitative properties before attempting to do more precise quantitative analysis and better to distinguish shapes by understanding them qualitatively rather than doing data base comparisons. Thus, methods will be developed to compute, in a timely, robust, and trustworthy manner, the fundamental geometric properties that any realistic mathematical model associated to a given data set must contain. Then statistical and analytic techniques may be applied to the geometrically correct models in order to extract the detailed information desired by practitioners.

====================

Manifold learning and dimensionality reduction: In the last decades, a set of new geometric methods, known as manifold learning, has been developed in the machine learning community. They are based upon the assumption that the data lie on a submanifold M in Rd and they mainly focus on nonlinear dimensionality reduction of the data set. Some of them, like principal curves [HS89] or generative topographic mapping (GTM) [BSW98], aim at fitting the data set by a parameterized low dimensional (in general 1 or 2) manifold. They usually assume that the topology of the manifold is known and simple (simple curves, planes, discs) and do not deal with data sampling more complicated shapes. Other methods, such as Multidimensional Scaling, LLE or ISOMAP[RS2000, RS2003, TSL2000], intend to find “projections” of the data onto lower dimensional Euclidean spaces that respect various geometric properties of the data. Again, these methods assume very restrictive hypothesis on the geometry of the manifolds sampled by the points to ensure correctness. Geometric diffusion and computational harmonic analysis methods have also been proposed to recover geometric properties of the underlying manifold [CLLMNWZ05]. These approaches consist of projecting the data set onto a low dimensional space generated by the first eigenvectors of a diffusion operator defined on a graph built from the data. They are rather sensitive to the quality of the sampling, and the geometric interpretation of the properties of the projected data is not always clear. From a computational point of view, a common drawback of these methods is that they usually involve computations of eigenvalues and eigenvectors of matrices of the size of the data set, preventing to deal with huge data sets without a pre-processing.

============
The recognition of objects in potentially complex scenes is one major issue in Computer Vision, that weights billions of dollars each year [Lowe]. Prominent advances integrate two essential stages: the induction of features of limited size, over a potentially huge feature space, and the use of these features for learning and classification. This approach is extremely popular, as it is also supported by biological evidences for the hierarchical treatment of vision, beginning from low­level descriptors [Poggio]. Biological evidences also support the integration of particular distortions in vision, that are precisely Bregman divergences [Poggio]. In fact, distortions are used throughout all the process of object recognition in vision: the detection of features using statistical, algebraic or geometric techniques, and the use of these features throughout classifiers.

=============

To achieve this task, we intend to take advantage of recent developments in the theory of topological persistence, which abstract themselves from the classical Euclidean setting and consider the input data as mere point clouds coming from finite or compact metric spaces. On the algorithmic side, we will need to use data structures that do not rely on a particular embedding of the data, but rather on its intrinsic metric. Such structures include the Rips and witness complexes, among others, and studying or limiting their complexity in general settings will be one of the main challenges, alongside with providing lightweight and efficient implementations for practical use

===============

Our long-term vision is to settle   the foundations and develop the algorithmic infrastructure 
on which applications of Geometric Modeling in higher dimensions will be made possible.

================

We believe such pursuits can be the key to breaking current and future computational bottlenecks in many areas of engineering. 

================

Computational geometry enjoys two unique assets: (1) its diversity and potential to affect most forms of computing; (2) its mature algorithmic foundations. The challenge is now to make this potential come true, and build an effective pipeline connecting theory to practice. We believe that this will revitalize the field and open new vistas for geometric research, both of a practical and a theoretical nature. The solutions of the most exciting open problems might be unknown. But the most exciting open problems themselves might be unknown, too. We are confident that creating the pipeline will unveil many of these problems. 


===============

A distinctive feature is the design and implementation of novel algorithmic solutions with certified topol- ogy and numerics as an alternative for heuristics and ad hoc methods, and the development of an experimental geometry kernel for modeling and computing with complex shapes as a proof-of-concept justifying our ap- proach. The results of this project should be directly useful to the application areas mentioned above. We intend to disseminate our work by publication in the appropriate applied research forums, by organizing multidisciplinary workshops aimed at exchange of knowledge and discussion of our work. Moreover, we aim at transferring our new technology by producing high quality software, demonstrating the feasibility of our techniques in practice. Cooperation with our industrial partner includes the assessment, trial, validation and packaging of the software developed in the project, thus guaranteeing a smooth transfer of new technology to application areas.

\end{document}

\section{Extended synopsis of the project}

\paragraph{The need to understand higher-dimensional spaces and shapes}
% 3D shapes are now ubiquitous and there is no need to motivate 
%  % with applications ranging from numerical simulations, visualization, reverse engineering, and in so many fields
% % medical imaging, cultural heritage, 
% that it may be considered strange that the huge amount of techniques developed for modeling 3D
% shapes has not yet found extensions to higher dimensions. This project is built on the conviction that it is time to remedy to this state of affairs.

% Geometric modeling refers to the construction and the manipulation of computerized representations of shapes. Since shape is the first and foremost attribute of 
% our familiar 3D world, geometric modeling has been and still is a core discipline with a vast array of applications ranging from multimedia to engineering through computer-aided medicine. It has been  on the research agenda of several communities like  computer-aided design, computer graphics, computer vision, computational geometry, geometry processing and numerical analysis.

% Until now, most of the work on geometric modeling has been restricted to 3D shapes. However, 
%The need to represent higher-dimensional spaces and shapes 
is ubiquitous in science.  Physicists are used to combining space and time into a single 4-dimensional {\em space-time} continuum.  In particle physics, the {\em phase space} consists of all possible values of position and momentum variables and is $6N$-dimensional.  {\em Configuration spaces} of mechanical systems, {\em conformational spaces} of macromolecules are other examples of common high-dimensional geometric objects.  Other examples can be found in the analysis of  dynamical systems. Although their needs may be very different, scientific computing, motion planning, molecular docking are among celebrated applications where approximating and processing high-dimensional objects is a central issue. 

A maybe less intuitive place where one needs to understand  high-dimensional geometry is in {\em data analysis}. Natural and artificial systems like biological or sensor networks are often described by a large number of real parameters; a collection of text documents can be represented as a set of term frequency vectors in Euclidean space; similar interpretations can be given for image and video data~\cite{sl-mwp-2000}.  Data analysis can then be turned into a geometric problem by encoding those heterogeneous data as clouds of points in high dimensional spaces equipped with some appropriate metric. Usually those points are not distributed in the whole embedding space but, due to the very nature of the system that produced those data, lie close to some subset of much smaller intrinsic dimension. Hence, the data conveys some geometric structure whose extraction and analysis are key to understanding the underlying system.
Since data are produced at an unprecedented rate in all sciences, 
%modeling complex high-dimensional shapes and extracting geometric information from point clouds in high-dimensional spaces have become core tasks in science and engineering. 
understanding the geometry of high-dimensional point clouds has become a core task in science and engineering. 


\paragraph{The curses of higher-dimensional geometry.} 

High-dimensional geometries are much more difficult to process and analyse than 3D shapes. The dimensionality severely restricts our intuition and ability to visualize the data. Hence  understanding higher dimensional shapes must rely on automated tools able to extract useful qualitative and quantitative information from the input high-dimensional data.

Moreover, the complexity of the data structures and of the algorithms used to  approximate shapes  rapidly grows as the dimensionality increases, which makes them intractable in high dimensions.  This curse of {\em dimensionality} is exemplified by the size of one of the simplest representations of a point set, namely its convex hull, whose complexity depends exponentially on the dimension. 
%last sentence can be removed

In addition, high-dimensional data often suffer from significant {\em defects}, including sparsity, noise, and outliers  which make them much more difficult to process than the 3D data routinely provided by scanning devices. This is particularly so in the case of biological data, such as high throughput data from microarray or other sources. Moreover, the structure and occurrence of geometric features in the data may depend on the {\em scale} at which it is considered, thus requiring the analysis process to be multiscale.  

\paragraph{The emergence of computational topology.}
The last decade has seen tremendous progress in  the understanding of geometry in high-dimensional spaces. In robotics~\cite{sml-pa-2006}, randomized techniques have been proposed to capture the topology of configuration spaces and to search paths. In signal and image processing, and in machine learning, a variety of techniques have been proposed to reduce the dimension of data, learn nonlinear manifolds and cluster data~\cite{hs-fmmds-2006}. % In computational geometry, new approaches have been proposed to solve basic problems like searching nearest neighbours, computing smallest enclosing ellipsoids or approximating convex sets~\cite{hp-gaa-2011}.
Those techniques are widely used but have limited guarantees and 
%address elementary questions (which may be very hard to solve though) and 
impose strong constraints on the dimension or the topology of the shapes they can successfully handle. Computational topology emerged as a new discipline  to 
provide complementary techniques for understanding complex shapes~\cite{hh-ct-2010}. 
The concepts of $\varepsilon$-samples, restricted Delaunay triangulation, anisotropic meshes emerged together with the first efficient and provably correct algorithms for emblematic problems like mesh generation and shape reconstruction in 3-dimensions~\cite{geometrica-ecg-book}. 

Attempts to analyze higher dimensional shapes led to the development of beautiful pieces of theory with deep roots in various areas of mathematics like Riemannian geometry, geometric measure theory, differential and algebraic topology. Let us mention  the emergence of a sampling theory of geometric objects and of geometric inference~\cite{geometrica-ccl09}, and the groundbreaking invention and rapid growth of persistent homology~\cite{eh-ph-2008}.
Although of a fundamental nature, these advances 
attracted  interest in several fields like data analysis, computer vision and sensor networks.

\paragraph{The grand challenge~: settling the algorithmic foundations.}

 % Although there is a crucial need for geometric modeling tools in higher dimensions, the theoretical results obtained so far  only led to prototype codes, and applications have been limited to low dimensions or to rather crude techniques. 

%The project addresses fundamental issues in an emerging field with huge potential impact.
% it has not yet penetrated applied fields due to the lack of satisfactory algorithmic solutions in high-dimensional spaces. 
% This project is intended to remedy to this situation by proposing  a long-standing alliance between mathematical developments, algorithmic design and advanced programming.


However, the current theory has not demonstrated its scalability to real problems and, up to now, it has only been applied to rather simple cases and in low dimensions. This is  due to the lack of satisfactory algorithmic solutions in high-dimensional spaces.
Breaking the computational bottleneck is now the main issue.  Settling the {\em algorithmic foundations} of geometry understanding in
higher dimensions
%al geometric modeling 
is a grand challenge of great theoretical and practical significance.


The tenet of this proposal is that, to take up the challenge, we need a global approach involving
tight and long-standing interactions between mathematical developments, algorithmic design and advanced programming. We believe that this is key to obtaining methods with built in
robustness, scalability and guarantees, and therefore  impact in the long run.
Such an approach has been successfully carried out in low dimensions with the 
development of the Computational Geometric Algorithms Library CGAL~\cite{cgal}. 

%We want to build upon this success 
By following this paradigm, we want  to give to  geometry understanding in higher dimensional an effective theory and a reference software platform. 
We strongly believe that this ambitious objective is realistic and can be reached. To pave the way towards this goal, we have identified  four main scientific challenges.

% \paragraph{The emerging field of structural data analysis.} {\em Early geometric approaches, 
% dimension reduction, NL manifold learning, computational topology, geometric inference
% 3d data processing, surface reconstruction, simplification. }


\paragraph{Scientific challenge 1 :  Choosing the right representation.}
%Going beyond affine models and Euclidean spaces.}

% Standard methods like PCA in Machine Learning to deal with high dimensional data assume that the data can be well approximated by some affine subspace of small dimension.  To overcome this 

As discussed above, dimensionality reduction techniques cannot provide precise approximation of complicated shapes (as required in scientific computing) nor compute essential features of a shape like its topological invariants.
% In the last decades, a set of new geometric methods, known as manifold learning, have been developed with the intent of parametrizing nonlinear shapes embedded in high-dimensional spaces. Although widely used, those methods  assume very restrictive hypotheses on the geometry of the manifolds sampled by the datapoints to ensure correctness. 
More expressive representations of  highly nonlinear shapes are provided % , inspired by what has been done in 3-dimensions, consists in approximating
by {\em simplicial complexes}, the analogue of triangulations in higher dimensions. Recent research has exhibited sampling conditions under which  topological or geometric properties, or even a full approximation of the sampled shape can be recovered~\cite{geometrica-7142i,geometrica-ccl09,nsw-fhm-2008}. 
%Progress in higher-dimensions so far has been mostly theoretical and a  huge gap  remains to be %filled before  having at one's disposal fully satisfactory solutions of practical significance.% One research direction is to invent simplicial complexes of small complexity and easy to compute that still capture the main features of the underlying shape. see Attali and Carlsson. 
Many types of simplicial complexes exist. 
A fundamental issue is the choice of a metric which determines the type and quality of an approximation.
 The simple Euclidean distance in the ambient space, while easy to deal with, is often not the right choice.  As already mentionned, most of the time, when working in high dimensional spaces, we are in fact interested in objects of much smaller intrinsic dimension. The intrinsic geometry of the objects provides the right framework. Computational intrinsic geometry has not been seriously tackled yet and a basic question like the existence of  Delaunay triangulations on Riemannian manifolds is still open.  This question is of utmost importance for anisotropic mesh generation and optimal approximations. Another important situation is when the data are not given as a point cloud in some Euclidean space, but rather as a matrix of pairwise distances (i.e. a discrete metric space). Although such data may not be sampled from geometric subsets of Riemannian manifolds, it may still carry some interesting topological structures that need to be understood. 

In information theory, signal and image processing, other pseudo-distances such as Kullback-Leibler, Itakura-Saito or Bregman divergences are preferred.  
%  In computer  vision, divergences are used throughout all the process of object recognition: to detect features using statistical, algebraic or geometric techniques, and to use these features throughout classifiers.
%The recognition of objects in potentially complex scenes is a major issue in Computer Vision. 
% Prominent advances in object recognition integrate two essential stages: the induction of features of limited size, over a potentially huge feature space, and the use of these features for learning and classification. 
These divergences are usually not true distances (they may not be symmetric nor satisfy the triangular inequality) and it is necessary to revisit geometric data structures and algorithms in this context~\cite{geometrica-6154a}.




\paragraph{Scientific challenge 2 :  Bypassing the curse of dimensionality.} 
Simplicial complexes have been known and studied for a long time in mathematics.  
Still the simplicial complexes required to approximate complicated shapes in high dimensions are huge objects and their construction is problematic. In fact, 
the complexity of many geometric algorithms and data structures grows exponentially with increasing dimension. %This behavior is commonly called the curse of dimensionality after Bellman.  An immediate consequence is that it is no longer possible to partition a high dimensional space. 
This rules out most, if not all, geometric algorithms developed in low dimensions.  Hence, extending computational geometry in high dimensions cannot be done in a straightforward manner and one has to take advantage of additional structure in the problem or  restrict attention to approximations. This motivated a number of new concepts (e.g. core sets), new algorithmic paradigms (e.g. locality-sensitive hashing) and new analyses (e.g. smoothed analysis).  In this project, we will address the curse of dimensionality by focusing on the inherent structure in the data which in some sense needs to be sparse or of low intrinsic dimension~\cite{sl-mwp-2000} as well as by putting the emphasis on output-sensitive algorithms and average-case analysis.  
First investigations led to very promising results, such as the design of new simplicial complexes with good complexity and approximation algorithms that scale well with the dimension \cite{geometrica-7142i,cds-tewc-2004}.


% An important feature is that, even if we go to approximate solutions, we do not want to sacrifice guarantees. This is mandatory since the behaviour of algorithms in high dimensions is much less intuitive and easy to predict than in small dimensions. %\framebox{Numerics?} 





% The intrinsic geometric structure is often not easy to capture. In some situations the object is implicitly known and one can query the object of interest using appropriate oracles to get new information  (black-box model). In other situations like in Data Analysis, detecting the hidden structure is part of the problem. 

\paragraph{Scientific challenge 3 : Searching for stable models.} 
When dealing with approximation and samples, one needs stability results to ensure that the quantities that are computed are good approximations of the real ones. This is especially true in higher-dimensions where data are usually corrupted by various types of noise.  When the noise is of small amplitude, methods have been proposed to robustly estimate topological and geometric properties of shapes~\cite{geometrica-ccl09,nsw-tvu-2011}. 
 The recent and fast developing theory of {\em persistent homology} provides a powerful tool to study  the homology of sampled spaces~\cite{eh-ph-2008}.
However, in  many applications the noise is non local and the previous methods fail.
Recently,  larger families of noise models  have been considered and statistical approaches  have been proposed to approximate shape under  those models~\cite{gpvw-mme-2011}. These methods however do not provide topological guarantees on the approximation and do not always provide explicit estimates. A major challenge is thus to design  unifying frameworks that embrace statistical approaches and deterministic methods, and offer topological guarantees.   Statistical techniques are also needed to automatically select the relevant scales at which the geometry of data should be considered.

%A number of groundbreaking new approaches appeared recently. % Given a point cloud in Euclidean space, the approach consists in building a simplicial complex whose elements are filtered by some user-defined function. Under reasonable conditions on the input point cloud, and modulo a right choice of filter, the most persistent invariants in the filtration correspond to invariants of the space underlying the data~\cite{geometrica-cseh-07}. Thus, the persistence algorithm allow to recover global information from noisy data.
% {\em Non-local noise} and outliers can also be considered as empirical measures. A notion of distance to such measures has been defined and shown to be stable with respect to perturbations of the measure \cite{ccsm-gipm-2011}. A big challenge is to find efficient algorithms in arbitrary dimensions to compute or approximate the topological structure of the sublevel-sets of the distance to a measure. Such algorithms would naturally find applications in topological inference in the presence of significant noise and outliers, but also in other less obvious contexts such as stable clustering. 

% {\em Multiscale reconstruction} is another novel approach~\cite{geometrica-bgo-09}. Taking advantage of the ideas of persistence, the approach consists in building a one-parameter family of simplicial complexes approximating the input at various scales. %  It was shown that, for a sufficiently dense input data set, the family contains a long sequence of complexes that approximate the underlying space provably well, both in a topological and in a geometric sense. In fact, there can be several such sequences, each one corresponding to a plausible reconstruction at a certain scale. Thus,
%  Determining the topology and shape of the original object reduces to finding the stable sequences in the one-parameter family of complexes. Despite its nice features, multiscale reconstruction, in its current form, can only be applied to low-dimensional data sets in practice. 



\paragraph{Scientific challenge 4 : Turning theory into practice.}%Building up the reference platform for high-dimensional geometric modeling.}
A major challenge, if not the most important, is to develop theory that is of practical significance for applications.   To take up the challenge, we will foster a symbiotic relationship between theory and practice, and  undertake the development of a software platform in the spirit of the CGAL project. We consider such a platform as central to our research  for three main reasons.  {\em First}, the software platform will allow experimentation at a large scale, which is mandatory to design the right models and data structures. We believe that this will revitalize the current theory and open new vistas for research, both of a practical and a theoretical nature, leading towards a virtuous circle between theory and experimental research. This has proven to be of utmost importance when developing the CGAL library and will be even truer in high dimensional geometry.

{\em Second}, maintaining such a platform will help further effort and consolidation in the long run.  Having a library with interoperable modules will allow us to incrementally add more and more sophisticated tools based on solid foundations.  This is consistent with our long-term vision and our conviction that it is only through such a long standing effort that true impact, both theoretical and applied, can be gained.

{\em Third}, the platform will serve as a unique tool to communicate with the computational geometry community and with researchers from other fields. 
 In return, we will get feedback from practitioners which will help shape the theoretical models and the software platform.

\paragraph{Objectives and methodology.}
The central goal of this proposal is to settle the {\em algorithmic foundations of an effective theory of geometry understanding in higher dimensions}.  We aim at processing general highly nonlinear geometries with nontrivial topologies represented as {\em simplicial complexes}. We will bypass the computational bottleneck by exploiting the {\em intrinsic properties} of the objects of interest which can be modeled as {\em low-dimensional manifolds} in many applications. We intend to develop {\em practical algorithms} to mesh or reconstruct highly nonlinear manifolds, and to infer geometric and topological properties from data subject to significant {\em defects} and under {\em realistic conditions}. This proposal adresses {\em fundamental
  research} issues, but its results are expected to serve as a basis
for groundbreaking advances for {\em applications in scientific computing
and data analysis}.  A major outcome of the project will be a
high-quality open source software {\em platform} of components
implementing the main results.

To reach these objectives, we will follow the principles that have been
guiding my research for more than 15 years and will simultaneously
develop {\em mathematical approaches} providing theoretical
guarantees, {\em effective algorithms} that are amenable to
theoretical analysis and fully validated experimentally, and {\em
  perennial software} development.


The proposal is structured into the following four {\em focus areas} that address the four scientific challenges above~:
{\bf FA1}:  {\em Dimension-sensitive data  structures} will extend current knowledge about simplicial complexes, and  provide efficient data structures and basic algorithms for their representation, construction and manipulation. 
  {\bf FA2}:  {\em Triangulation of non Euclidean geometric spaces} will develop effective algorithms to mesh or reconstruct manifolds equipped with various metrics.   {\bf FA3}: {\em Robust models for homology inference, comparison and  clustering} will provide the crucial  algorithms for topological data analysis.
 {\bf FA4}:  {\em  Software platform for geometric understanding in high dimensions} will provide the software environment for experimenting with our new data structures and algorithms, for integrating them in a coherent library of interoperable modules, and for diffusing our results to applied fields. 

\paragraph{Risks and feasibility of the project.} 
Simultaneously pursuing basic research at the best international level and developing software of the highest quality is not without risks. % On one side, there is a risk to stick to heuristics solutions and software with no guarantees. The opposite pitfall would be to develop beautiful theoretical techniques with little impact outside of academic circles. 
My personal record as well as the record of the members of my research group Geometrica who will be involved in this project are strong indications of our ability to take up the challenge with good chances of success.  Geometrica has been at the cutting edge of research in geometric data structures and algorithms~\cite{geometrica-ecg-book,by-ag-98}, mesh generation, manifold reconstruction~\cite{geometrica-7142i,geometrica-bgo-09}, geometric inference and computational topology~\cite{geometrica-ccl09}--\cite{geometrica-cseh-07}. Geometrica is also one of the leader teams of the CGAL project~\cite{cgal}.  We were at the source of successfull developments in CGAL like interval arithmetics, triangulations (now integrated in the heart of Matlab) and meshing packages. We also took a prominent part in the animation of the CGAL project and community, and in the creation of the spinoff GeometryFactory. It can be argued that  my research group Geometrica is the best team worldwide to take up this dual challenge and to make this project a success. 


%The research won't be conducted in isolation.  
We will benefit from our strong collaborations with the best groups in Europe and in the US.
Research in computational geometry and topology is very active in  Europe.  The ICT Fet-Open project Computational Geometric Learning (CG-Learning) is closely related to this project. The focus, the timetable and the management though are different. The proposed project wants to take over the results of CG-Learning and to go beyond prototype developments.  


%We will also benefit from our long-standing collaborations in the USA with Stanford university %(Pr. Guibas) and Ohio State university (Pr. Dey) on topics that are related to this project.



\paragraph{New horizons and opportunities.} 



% Computational geometry and topology have produced beautiful pieces of theory and. Although many of these tools are of great potential impact in applications, implementing those ideas in robust and efficient codes will only be possible through a long-standing alliance between mathematical developments, algorithmic design and advanced programming.  This last point is underestimated and implementation is often considered as an engineering activity that can be let to students. I believe that this is a wrong vheartiew that largely explains the current absence of  reliable software toolbox for geometric modeling in higher-dimensions. % My conviction, built after more than ten years of development of CGAL, is  that the suggested workplan is the only way to make  impact in the long run.

If successful, the project will put higher-dimensional geometric modeling on new theoretical and algorithmic ground. It will help setting up a first class research group with a unique spectrum of expertise covering mathematics, algorithm design and software development.
This new project will further strengthen the leadership of Europe  in Geometric Computing.
It will also provide an open platform with no equivalent in USA or Asia.
We foresee the project becoming a catalyst for research in high-dimensional geometry inside and outside of the project.  
By implementing the most effective techniques in a  reliable and scalable way, the platform will
open the way to groundbreaking technological advances in scientific computing and data analysis for applications as varied as numerical simulation, computer vision, robotics or molecular biology. We will keep close contacts with research groups working in those domains and leap on opportunities arising from our new results and tools.

{\footnotesize
\bibliographystyle{abbrv}
\bibliography{erc}
}



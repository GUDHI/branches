% -*- LaTeX -*-
%
% 20120205
%


% intrinsic Dts
%

The Delaunay paradigm has proven to be central to the development and
understanding of meshing algorithms, whether the domain of interst is
a full dimensional subset of $\rdee$, or a more general manifold. In
order strengthen the theoretical foundations of anisotropic meshing of
Euclidean domains, and of meshing general Riemannian manifolds, we
believe it is important to develop a deeper understanding of the
Delaunay complex defined by a Riemannian metric. 

We now know that sampling conditions exist which can guarantee that
the intrinsic Delaunay complex is a triangulation of a compact
manifold, and that these conditions depend not just on the sampling
density, but also on a genericity condition. However, these conditions
have only been developed with respect to extrinsic criteria, when the
manifold is supposed to be embedded in $\rdee$. Specifically, the
sampling density condition relates extrinsic distances to the
\emph{reach} of the manifold, a quantity that has no meaning in an
intrinsic context, and the genericity condition relies on an
orthogonal projection onto the tangent space. 

We aim to develop purely intrinsic sampling criteria. We have shown
that if the Delaunay balls are sufficiently \emph{protected}, with
respect to the Euclidean metric, then locally the Delaunay
triangulation will be the same if the Riemenannian metric is close to
the Euclidean one, where the closeness is defined with respect to the
scale of the sampling. This means in particular that the intrinsic
Delaunay triangulation is locally a triangulation. However, the result
does not hold if we instead define the protection directly in terms of
the intrinsic Riemannian metric. It is a significant gap in our
understanding that we cannot express the required genericity condition
in terms of the intrinsic metric. It makes the required sampling
condtions awkward, and it unnaturally constrains our sampling density
requirements: we would require that the density is bounded with
respect to the absolute value of the sectional curvatures (in order to
obtain small metric distortion between local Euclidean metrics),
whereas we expect that a sampling density bounded by the strong
convexity radius should be possible.

After establishing the local criteria we need for correctness, we will
demonstrate that the resulting simplicial complex is homeomorphic to
the manifold. For this we cannot use the established techniques
developed for embedded manifolds, nor arguments based on the Voronoi
diagram itself, since the genericity criteria do not imply that the
closed ball property will be met. Instead we will build from techniques
described by Munkres and Cheeger. The resulting homeomorphism theorem
should be general enough to be used in other contexts.

Finally, we will develop a meshing algorithm based on these
results. This will likely use ideas from controlled perturbation, as
well as the witness complex. The intrinsic Delaunay triangulation is
an abstract simplicial complex. In order to turn it into a mesh, we
need to endow the simplices with a Euclidean structure. The obvious
thing to do is to give the edges the lengths of the geodesics between
the corresponding vertices. We will need to establish that our
genericity conditions are sufficient to ensure that the Cayley-Menger
determinant of the resulting simplices is positive, and thus that the
edge lengths do describe well-defined Euclidean simplices.


% witness cplx algoritms
%

For the construction of Delaunay-type structures, the witness complex
has appeal because the complex is built from only a subset of the
sample points, and the required geometric predicates involve only
distance comparisons, rather than the evaluation of polynomials whose
degree depends on the dimension of the space. 

However, a more careful argument is required if we are to establish
that either of these points represent a true benefit. For
triangulating manifolds, the sampling density of the \emph{landmarks}
will be required to be at least as high as the required sampling
density for other Delaunay based triangulation methods. So the fact
that only a subset of the points need be chosen, is more a reflection
that an extremely dense set of witnesses is required.

This leads to the other point: The geometric predicate in the Delaunay
triangulation is a determinant, a polynomial whose degree depends on
the dimension of the space. This must be evaluated every time a
simplex is selected for consideration. So how many times is that in a
randomized incremental Delaunay construction? By contrast the witness
complex only requires comparisons of distances, but how many times?

We have established that if the landmarks are chosen such that they
are $\delta$-generic, then there is a sampling density for the
witnesses that guarantees that every Delaunay simplex must be
witnessed. If the efficiency of construction is the primary motivation
for the witness complex in the context of manifold reconstruction from
dense point clouds, then this efficiency should be demonstrable in the
context of constructing Delaunay triangulations of Euclidean domains.

Specifically, given a set $\pts \subset \rdee$, satisfying a sampling
radius of $\epsilon$, we can describe a
controlled perturbation implementation of a randomized incremental
Delaunay triangulation algorithm, that will guarantee that the output
is a $\delta$-generic Delaunay triangulation of a point set
$\tilde{\pts}$ that is $\eta$-close to $\pts$ in the Hausdorff sense,
for some very small $\eta$. 

So the question is: can it possibly be more efficient to construct
this same complex $\text{Del}(\tilde{\pts})$ as a witness complex. We
know that for a sufficient density of witnesses, the witness complex
and the Delaunay complex will coincide. The witnesses can be supposed
to lie on cartesian grid points, for a sufficiently fine grid. Will
the number of distance comparisons required scale favorably with the
number of operations required for the incremental Delaunay
triangulation algorithm?

If this can be competitive, this algorithm can presumably be adapted
to the case of manifold reconstruction.

% sparse sampling
%

The reconstruction algorithms we consider require a sampling density
so extreme, that they might be better considered to be meshing
algorithms: the required point cloud needs to be sufficiently dense
that it can already be considered to be a representation of the
manifold.

In practice, we will wish to reconstruct manifolds from input point
sets that are so sparse that homeomorphism guarantees will be
impossible without additional prior knowledge of the unknown
manifold. 

This points to two research directions. First, is there a reasonable
set of priors that can allow us to recover homeomorphism guarantees
for much looser conditions on the sampling? In other words, can we
find a restricted class of Riemannian manifolds, that are
representative of many applications, but for which it is much easier
to distinguish between the isometry classes?

Second, in the absence of such restrictive assumpions on the input
manifold, can we establish alternative quantative quality measures for
the approximation quality of the output reconstruction that are still
informative? Estimates of the homology groups of the manifold is one
direction that has been taken, although it is not entirely clear that
the required sampling conditions are significantly less
restrictive. Ideas based on mass-transport appear interesting.
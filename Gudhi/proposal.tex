\section{Research proposal (15p.)}

\subsection{State-of-the-art and objectives}

\paragraph{The research and development pipeline in Computational Geometry.}

Computational geometry emerged as a discipline in the seventies and has met with considerable success in providing foundations to solve basic geometric problems including data structures, convex hulls, triangulations, Voronoi diagrams, geometric arrangements and geometric optimisation~\cite{by-ag-98}. This initial development of the discipline has been followed, in the mid-nineties, by a vigorous effort to make computational geometry more effective.  Situated in-between basic theoretical research and the development of robust software is the emerging study of effective methods (algorithms and data structures) of geometric computing, namely methods that are not only theoretically proved but also work well in practice.  Several EC projects (CGAL, GALIA) established an outstanding research momentum and gave a leading role to Europe in this context.  They led to successful techniques and tools, most notably the CGAL library~\cite{cgal}, a unique tool that provides a well-organised, robust and efficient software environment for developing geometric applications. CGAL is considered as one of the main achievements of the field and is by now the standard in Geometric Computing, with a large diffusion worldwide and varied applications in both academia and industry. CGAL has no equivalent counterpart in the world.

\paragraph{3D Geometric Modeling.}
Approximating complex shapes through meshing is a fundamental problem on the agenda of several communities like Numerical Analysis, Computer Graphics, Geometry Processing and Computer-Aided Design.  Emblematic problems are mesh generation that aims at sampling and meshing a given domain, and surface reconstruction that constructs an approximation of a surface which is only known through a set of points. Although these problems have received considerable attention in the past, it is only during the last 10 years that 
the Computational Geometry community established solid theoretical foundations to the problem, most notably in the emerging new area of Computational Topology. This approach has shown to be very successful and led to recent breakthroughs in {\em mesh generation} \cite{geometrica-bcmrv-ms-06} and {\em surface reconstruction} \cite{dey-csr-2007}.  The Geometrica group took a leading role in this research and contributed major theoretical advances as well as practical developments in the form of fast, safe and quality-guaranteed CGAL components for mesh generation and shape reconstruction~\cite{cgal:rty-m3-11}. Those components are now used worldwide in academia and in industry for various applications in Geometric Modeling, Medical Imaging and Geology.

\paragraph{Noisy data.}
When dealing with approximation and samples, one needs stability results to ensure that the quantities that are computed, geometric or topological invariants, are good approximations of the real ones. {\em Topological persistence} was recently introduced as a powerful tool for the study of the topological invariants of sampled spaces~\cite{eh-ph-2008,rg-bptd-2008}. Given a point cloud in Euclidean space, the approach consists in building a simplicial complex whose elements are filtered by some user-defined function. This filter basically gives an order of insertion of the simplices in the complex. The persistence algorithm, first introduced by Edelsbrunner, Letscher and Zomorodian \cite{elz-tps-2002}, is able to track down the topological invariants of the filtered complex as the latter is being built. As proved by Cohen-Steiner et al. \cite{geometrica-cseh-07}, under reasonable conditions on the input point cloud, and modulo a right choice of filter, the most persistent invariants in the filtration correspond to invariants of the space underlying the data. Thus, the information extracted by the persistence algorithm is global, as opposed to the locality relationships used by the dimensionality reduction techniques. In this respect, topological persistence appears as a complementary tool to dimensionality reduction. In particular, it enables to determine whether the input data is sampled from a manifold with trivial topology, a mandatory condition for dimensionality reduction to work properly. Note however that it does not tell how and where to cut the data to remove unwanted topological features.

Multiscale reconstruction is a novel approach~\cite{geometrica-bgo-09}. Taking advantage of the ideas of persistence, the approach consists in building a one-parameter family of simplicial complexes approximating the input at various scales. Differently from above, the family may not necessarily form a filtration, but it has other nice properties. In particular, for a sufficiently dense input data set, the family contains a long sequence of complexes that approximate the underlying space provably well, both in a topological and in a geometric sense. In fact, there can be several such sequences, each one corresponding to a plausible reconstruction at a certain scale. Thus, determining the topology and shape of the original object reduces to finding the stable sequences in the one-parameter family of complexes.   However, multiscale reconstruction, at least in its current form, still has a complexity that scales up exponentially with the dimension of the ambient space. Hence, it can only be applied to low-dimensional data sets in practice.


\paragraph{High dimensional spaces.}
Dimensionality reduction is certainly one of the most popular approaches to high-dimensional data analysis. It consists in projecting the data points down to a linear subspace, whose dimension supposedly coincides with the intrinsic dimension of the data. This approach is elegant in that it helps detect the intrinsic parameters of the data, and by doing so it also reduces the complexity of the problem. Dimensionality reduction techniques fall into two classes: linear methods, e.g. principal component analysis (PCA) or multi-dimensional scaling (MDS), and non-linear methods, e.g. isomap or locally-linear embedding (LLE). The second class of algorithms is more powerful in that it computes more general (in fact, non-linear) projections. On the whole, dimensionality reduction works well on data sets sampled from manifolds with low curvature and trivial topology. Although the condition on the curvature is mainly a sampling issue, the condition on the topology is mandatory for the projection onto a linear subspace to make sense.


Many of the results of Computational Geometry have been extended to arbitrary dimension. In particular, worst-case optimal algorithms are known for computing convex hulls, Voronoi diagrams and Delaunay triangulations in any dimension. Hence, in principle, the methods developed for 3D applications should be extendable to higher dimensions. However, the size of these structures depends exponentially on the dimension of the embedding space, which makes them only useful in moderate dimensions~\cite{geometrica-6743i}. Many ideas have been suggested to bypass this curse of dimensionality. A first approach looks for more realistic combinatorial analyses such as smoothed analysis that bounds the expected complexity under some small random perturbation of the data. This led to the celebrated analysis of Linear Programming of Spielmann and Teng \cite{st-saa-2004}. Another route is to trade exact for approximate algorithms \cite{hp-gaa-2011}.  An important example is the search for approximate nearest neighbours. Another example is the theory of core sets that was shown to provide good approximate solution to some optimization problems like computing the smallest enclosing ball, or computing an optimal separating hyperplane (SVM). These tools are both extremely useful but limited to basic operations and have not been yet applied  in the context of geometric modeling. 
A third approach assumes that the intrinsic dimension  of the object of interest has a much lower intrinsic dimension than the dimension of the embedding space. This is the usual assumption in Manifold Learning. It is then possible to resort to techniques derived from the 3D case and to approximate complex shapes by simplicial complexes (the analogue of triangulations in higher-dimesnional spaces).  Various types of simplicial complexes have been proposed such as the Czech and the RIPS complexes,  and the more recent Delaunay-like complexes such as the $\alpha$-complex~\cite{eks-sspp-83,he-ubds-95}, the witness complex~\cite{deSilva2008,cds-tewc-2004} and the Delaunay tangential complex~\cite{geometrica-7142i}. They differ by their combinatorial and algorithmic complexities, and their power to approximate a shape.
Under appropriate sampling conditions, we have shown that one can reconstruct a provably correct approximation of a smooth $k$-dimensional manifold $M$  embedded in $\R ^d$ in a time that depends only linearly on $d$~\cite{geometrica-7142i}. Researchers have also turned their focus to the somewhat easier problem of inferring topological invariants of the shape without explicitly reconstructing it with the hope that more lightweight data structures and weaker sampling conditions would be appropriate for this simpler task~\cite{nsw-fhm-2008,co-tpr-2008}.


% A related concept is that of doubling dimension.  Intrinsic algorithms. Discrete metric spaces. Parametrized complexity.



% =====

% In [??], the family of complexes is derived from the so-called alpha-offsets of the input point cloud, defined as the unions of balls of same radius alpha centered at the data points. In [??], the family is derived from the so-called witness complex, first introduced by de Silva [??], which can be viewed as a weak version of the Delaunay triangulation whose construction requires much less computation power. Despite this nice feature, multiscale reconstruction, at least in its current form, still has a complexity that scales up exponentially with the dimension of the ambient space. Hence, it can only be applied to low-dimensional data sets in practice.


\subsection{Methodology}

Our overall goal is to settle the foundations for Geometric Modeling in higher dimensions by
developing 

\sind $\bullet$ Sounded approaches providing guarantees even in the presence of noise or outliers,


\sind $\bullet$  Algorithms that are of both theoretical and practical interest, that are amenable to theoretical analysis and fully validated experimentally,

\sind $\bullet$ A software platform that is generic, robust and efficient.

The proposal is structured into the following four workpackages:


\sind $\bullet$  {WP 1:  Dimension-sensitive algorithms and data  structures.} 


\sind $\bullet$  {WP 2:  Computational geometry in non euclidean spaces.}


\sind $\bullet$  {WP 3: Robust geometric models.}


\sind $\bullet$  {WP 4:  Platform for geometric modelling in higher dimensions. } 


\input{wp1}
\input{wp2}
\input{wp3}
\input{wp4}


\bibliographystyle{abbrv}
\bibliography{erc}

\section{Ressources}



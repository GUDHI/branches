\section{Research proposal (15p.)}

\subsection{State-of-the-art and objectives}

Geometric modeling has undergone huge progress during the past decades. The pressing needs
of multimedia, video games, numerical simulations, manufacturing, computer-aided medicine, culturage heritage and other applications asked for techniques to respresent, process and analyze
{\em 3D} shapes.  Progress has been contributed by  varied disciplines, most notably computer graphics, geometry processing, computer-aided design, computational geometry and topology, scientific computing. This considerable research effort resulted in settling theoretical and algorithmic foundations of 3D geometric modeling, and providing efficient algorithms and codes for applications such as surface reconstruction, mesh generation and point cloud processing~\cite{geometrica-bcmrv-ms-06,dey-csr-2007}.  
The situation is very different and much less has been done for higher dimensional shapes.%   are much more difficult to handle than surfaces in 3-space for two main reasons~: the curse of dimensionality and the unafordable presence of noise.


\paragraph{Dimension reduction.} A first route towards geometry processing in higher dimensional spaces is to try to reduce the dimension. This is 
one of the most popular approaches to high-dimensional data analysis. {\em Dimensionality reduction} consists in mapping the data points down to a linear subspace, whose dimension supposedly coincides with the intrinsic dimension of the data. This approach is elegant in that it helps detect the intrinsic parameters of the data, and by doing so it also reduces the complexity of the problem. Dimensionality reduction techniques fall into two classes: linear methods, e.g. principal component analysis (PCA) or multi-dimensional scaling (MDS), and non-linear methods, e.g. isomap or locally-linear embedding (LLE). The second class of algorithms is more powerful in that it computes more general (in fact, non-linear) mappings. On the whole, dimensionality reduction works well on data sets sampled from manifolds with low curvature and trivial topology. Although the condition on the curvature is mainly a sampling issue, the condition on the topology is mandatory for the mapping onto a linear subspace to make sense. \framebox{Johnson-Lindenstrauss, Wakin}

\paragraph{Simplicial complex approximations.}
In many problems \framebox{examples?}, a linear subspace or a union of linear subspaces is a too crude model and we need to represent nonlinear geometries like a $k$-dimensional submanifold of $\R^d$.  To deal with such complicated geometries, a route is to approximate shapes by simplicial complexes.  Simplicial complexes have been introduced by Poincar\'e in the early days of algebraic topology. Their importance in low dimensions cannot be overerestimated~: 1-dimensional simplicial complexes, better known as graphs or networks, can be found everywhere, 2-dimensional simplicial complexes are the standard representations for surfaces in computer graphics and 3-dimensional simplicial complexes are the representation of choice for scientific computing and numerical simulation when complicated domains are involved. Simplicial complexes can be defined in any dimension and can be used to construct a piecewise linear approximation of a shape with the right topological type (see WP2) or to 
compute the homology of the shape (see WP 3). The main bottleneck in using simplicial complexes in higher dimensions is of a computational nature~:  in high dimensions, simplicial complexes can be huge and very difficult to compute, which limits their current use to low dimensions.

\paragraph{Computational issues.}
The high computational cost of simplicial complexes can be balanced in two ways. First, simplicial complexes are flexible enough to approximate subspaces with a complexity that scales with the complexity of the subspace, not of the ambient space. Since the subspaces of interest 
 in learning theory, data analysis or dynamical systems have usually a small intrinsic dimension,
simplicial complexes may be an option.

The second reason is that coarse topological features such as the homology groups or the Betti numbers (that are invariant under homotopy equivalence) can be estimated from simplicial complexes of a much smaller size than what is needed to reconstruct a homeomorphic approximation. Hence, simplicial complexes may be used to recover important information even if an accurate representation of the data is out of reach.

Simplicial complexes thus have the potential of being the data structure of choice for understanding geometry in high dimensional spaces. The issue is however to define simplicial complexes that are dimension-sensitive and easy to compute.  Various types of simplicial complexes have been proposed such as the \v{C}ech and the Rips complexes, and the more recent Delaunay-like complexes such as the $\alpha$-complex~\cite{eks-sspp-83,he-ubds-95}, the witness complex~\cite{deSilva2008,cds-tewc-2004} and the Delaunay tangential complex~\cite{geometrica-7142i}. They differ by their combinatorial and algorithmic complexities, and their power to approximate a shape.  The algorithmic theory of simplicial complexes is nevertheless in its infancy and much less understood than the computational aspects of triangulations in small dimensions. In particular, little is known about efficient and compact representations of simplicial complexes and experiments have only been reported for simplicial complexes of low-dimensions up to now ~\cite{rg-bptd-2008}. \framebox{true?} 




%  Under appropriate sampling conditions, we have shown that one can reconstruct a provably correct approximation of a smooth $k$-dimensional manifold $M$  embedded in $\R ^d$ in a time that depends only linearly on $d$~\cite{geometrica-7142i}. Researchers have also turned their focus to the somewhat easier problem of inferring topological invariants of the shape without explicitly reconstructing it with the hope that more lightweight data structures and weaker sampling conditions would be appropriate for this simpler task~\cite{nsw-fhm-2008,co-tpr-2008}.







% Many of the results of Computational Geometry have been extended to arbitrary dimension. In particular, worst-case optimal algorithms are known for computing convex hulls, Voronoi diagrams and Delaunay triangulations in any dimension. Hence, in principle, the methods developed for 3D applications should be extendable to higher dimensions. However, the size of these structures depends exponentially on the dimension of the embedding space, which makes them only useful in moderate dimensions~\cite{geometrica-6743i}. Many ideas have been suggested to bypass this curse of dimensionality. A first approach looks for more realistic combinatorial analyses such as smoothed analysis that bounds the expected complexity under some small random perturbation of the data. This led to the celebrated analysis of Linear Programming of Spielmann and Teng \cite{st-saa-2004}. Another route is to trade exact for approximate algorithms \cite{hp-gaa-2011}.  An important example is the search for approximate nearest neighbours. Another example is the theory of core sets that was shown to provide good approximate solution to some optimization problems like computing the smallest enclosing ball, or computing an optimal separating hyperplane (SVM). These tools are both extremely useful but limited to basic operations and have not been yet applied  in the context of geometric modeling. 
% A third approach assumes that the intrinsic dimension  of the object of interest has a much lower intrinsic dimension than the dimension of the embedding space. This is the usual assumption in Manifold Learning. It is then possible to resort to techniques derived from the 3D case and to approximate complex shapes by simplicial complexes (the analogue of triangulations in higher-dimesnional spaces).  Various types of simplicial complexes have been proposed such as the Czech and the RIPS complexes,  and the more recent Delaunay-like complexes such as the $\alpha$-complex~\cite{eks-sspp-83,he-ubds-95}, the witness complex~\cite{deSilva2008,cds-tewc-2004} and the Delaunay tangential complex~\cite{geometrica-7142i}. They differ by their combinatorial and algorithmic complexities, and their power to approximate a shape.
% Under appropriate sampling conditions, we have shown that one can reconstruct a provably correct approximation of a smooth $k$-dimensional manifold $M$  embedded in $\R ^d$ in a time that depends only linearly on $d$~\cite{geometrica-7142i}. Researchers have also turned their focus to the somewhat easier problem of inferring topological invariants of the shape without explicitly reconstructing it with the hope that more lightweight data structures and weaker sampling conditions would be appropriate for this simpler task~\cite{nsw-fhm-2008,co-tpr-2008}.


% A related concept is that of doubling dimension.  Intrinsic algorithms. Discrete metric spaces. Parametrized complexity.



% =====

% In [??], the family of complexes is derived from the so-called alpha-offsets of the input point cloud, defined as the unions of balls of same radius alpha centered at the data points. In [??], the family is derived from the so-called witness complex, first introduced by de Silva [??], which can be viewed as a weak version of the Delaunay triangulation whose construction requires much less computation power. Despite this nice feature, multiscale reconstruction, at least in its current form, still has a complexity that scales up exponentially with the dimension of the ambient space. Hence, it can only be applied to low-dimensional data sets in practice.

\paragraph{Noisy data.} \framebox{to be revised}
When dealing with approximation and samples, one needs stability results to ensure that the quantities that are computed, geometric or topological invariants, are good approximations of the real ones. {\em Topological persistence} was recently introduced as a powerful tool for the study of the topological invariants of sampled spaces~\cite{eh-ph-2008,rg-bptd-2008}. Given a point cloud in Euclidean space, the approach consists in building a simplicial complex whose elements are filtered by some user-defined function. This filter basically gives an order of insertion of the simplices in the complex. The persistence algorithm, first introduced by Edelsbrunner, Letscher and Zomorodian \cite{elz-tps-2002}, is able to track down the topological invariants of the filtered complex as the latter is being built. As proved by Cohen-Steiner et al. \cite{geometrica-cseh-07}, under reasonable conditions on the input point cloud, and modulo a right choice of filter, the most persistent invariants in the filtration correspond to invariants of the space underlying the data. Thus, the information extracted by the persistence algorithm is global, as opposed to the locality relationships used by the dimensionality reduction techniques. In this respect, topological persistence appears as a complementary tool to dimensionality reduction. In particular, it enables to determine whether the input data is sampled from a manifold with trivial topology, a mandatory condition for dimensionality reduction to work properly. Note however that it does not tell how and where to cut the data to remove unwanted topological features.

Multiscale reconstruction is a novel approach~\cite{geometrica-bgo-09}. Taking advantage of the ideas of persistence, the approach consists in building a one-parameter family of simplicial complexes approximating the input at various scales. Differently from above, the family may not necessarily form a filtration, but it has other nice properties. In particular, for a sufficiently dense input data set, the family contains a long sequence of complexes that approximate the underlying space provably well, both in a topological and in a geometric sense. In fact, there can be several such sequences, each one corresponding to a plausible reconstruction at a certain scale. Thus, determining the topology and shape of the original object reduces to finding the stable sequences in the one-parameter family of complexes.   However, multiscale reconstruction, at least in its current form, still has a complexity that scales up exponentially with the dimension of the ambient space. Hence, it can only be applied to low-dimensional data sets in practice.


\paragraph{The research and development pipeline in computational geometry.}
Computational geometry emerged as a discipline in the seventies and has met with considerable success in providing foundations to solve basic geometric problems including data structures, convex hulls, triangulations, Voronoi diagrams, geometric arrangements and geometric optimisation~\cite{by-ag-98}. This initial development of the discipline has been followed, in the mid-nineties, by a vigorous effort to make computational geometry more effective by
combining in a closer way basic theoretical research and the development of robust software.
% is the study of effective methods (algorithms and data structures) of geometric computing, namely methods that are not only theoretically proved but also work well in practice.  
Several EC projects (CGAL, GALIA) established an outstanding research momentum and gave a leading role to Europe in this context.  They led to successful techniques and tools, most notably the CGAL library~\cite{cgal}  that provides a well-organised, robust and efficient software environment for developing geometric applications. CGAL is considered as one of the main achievements of the field and is by now the standard in geometric computing, with a large diffusion worldwide and varied applications in both academia and industry. CGAL is a unique library with no equivalent counterpart in the world.  My research group at INRIA took a leading role in the development of CGAL since its start more than 10 years ago. More recently, we seconded our basic research on  3D shape processing by practical developments in the form of fast, safe and quality-guaranteed software components for mesh generation and shape reconstruction that are now part of the open source library CGAL~\cite{cgal:rty-m3-11}. Those components are used worldwide in academia and in industry for various applications in geometric modeling, medical imaging and geology.


\paragraph{Overall objective.}
Time has come...

% Processing and analyzing complex 3D shapes is a fundamental problem with a long history of scientific successes which is on the agenda of several communities like scientific computing, computer graphics, geometry processing and computer-aided design.  Emblematic problems are mesh generation that aims at sampling and meshing a given domain, and surface reconstruction that constructs an approximation of a surface which is only known through a set of points. During the last decade, the computational geometry community established solid theoretical foundations to these problems leading to recent breakthroughs in {\em mesh generation} \cite{geometrica-bcmrv-ms-06} and {\em surface reconstruction} \cite{dey-csr-2007}.  The Geometrica group took a leading role in this research and contributed major theoretical advances as well as practical developments in the form of fast, safe and quality-guaranteed software components for mesh generation and shape reconstruction that are now part of the open source library CGAL~\cite{cgal:rty-m3-11}. Those components are now used worldwide in academia and in industry for various applications in Geometric Modeling, Medical Imaging and Geology.




\subsection{Methodology}
To reach our overall goal of settling firm algorithmic foundations for geometry understanding in higher dimensions, we will follow the principles that have been guiding my group for more than 10 years and will simultaneously develop
{\em mathematical approaches} providing guarantees even in the presence of noise or outliers,
{\em effective algorithms} that are amenable to theoretical analysis and fully validated experimentally,
and {\em perenial software} development.

The proposal is structured into the following four workpackages:
{\bf WP 1}:  {\em Dimension-sensitive data  structures} will extend current knowledge about simplicial complexes, and  provide efficient data structures and basic algorithms for their representation, construction and manipulation. 
  {\bf WP 2}:  {\em Triangulating non Euclidean geometric spaces} will develop effective algorithms to mesh or reconstruct manifolds and other spaces equiped with various metrics.   {\bf WP 3}: {\em Robust models for geometric inference, comparison and  clustering} will provide the crucial  algorithms for topological data analysis.
 {\bf WP 4}:  {\em  Software platform for geometric understanding in high dimensions} will provide the software environment for experimenting with our new data structures and algorithms, for integrating them in a coherent library of interoperable modules, and for diffusing our results to applied fields. We now describe in more detail each of these workpackages.


\input{wp1}
\input{wp2}
\input{wp3}
\input{wp4}


\bibliographystyle{abbrv}
\bibliography{erc}

\section{Ressources}

I will devote 70\% of my time to this project, and I will dedicate all my expertise and efforts to conduct and supervise the research work. To this end, I will receive the precious help of 2 permanent researchers of the Geometrica team : Fr\'ed\'eric Chazal who is a  leading researcher in geometric inference and computational topology and Mariette Yvinec who is an expert in geometric computing and a member of the CGAL Editorial Board. They will devote 20\% of their time to this project to co-supervise with me the research and implementation work of the students, postdocs and engineers to be engaged in this project. Other members of Geometrica,
not financially supported by this project, will also collaborate to the project.


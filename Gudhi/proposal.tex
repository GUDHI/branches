\section{Research proposal (15p.)}

\subsection{State-of-the-art and objectives}

Geometry understanding has undergone huge progress during the past decades. The pressing needs
of multimedia, video games, numerical simulations, manufacturing, computer-aided medicine, culturage heritage and other applications asked for techniques to represent, process and analyze
{\em 3D} shapes.  Progress has been contributed by  varied disciplines, most notably computer graphics, geometry processing, computer-aided design, computational geometry and topology, scientific computing. This considerable research effort resulted in solid theoretical and algorithmic foundations for 3D geometry understanding, and efficient algorithms and codes for applications such as surface reconstruction, mesh generation and point cloud processing~\cite{he-gtmg-2001,geometrica-bcmrv-ms-06,dey-csr-2007}.  
The situation is very different and much less has been done for higher dimensional shapes.%   are much more difficult to handle than surfaces in 3-space for two main reasons~: the curse of dimensionality and the unafordable presence of noise.


\paragraph{Dimension reduction.} A first route towards geometry processing in higher dimensional spaces is to try to reduce the dimension. This is 
one of the most popular approaches to high-dimensional data analysis. {\em Dimensionality reduction} consists in mapping the data points down to a linear subspace, whose dimension supposedly coincides with the intrinsic dimension of the data. This approach is elegant in that it helps detect the intrinsic parameters of the data, and by doing so it also reduces the complexity of the problem. Dimensionality reduction techniques fall into two classes: linear methods, e.g. principal component analysis (PCA) or multi-dimensional scaling (MDS), and non-linear methods, e.g. isomap or locally-linear embedding (LLE). The second class of algorithms is more powerful in that it computes more general (in fact, non-linear) mappings. On the whole, dimensionality reduction works well on data sets sampled from manifolds with low curvature and trivial topology. Although the condition on the curvature is mainly a sampling issue, the condition on the topology is mandatory for the mapping onto a linear subspace to make sense. \framebox{Johnson-Lindenstrauss, Wakin}

\paragraph{Simplicial complex approximations.}
In many problems \framebox{examples?}, a linear subspace or a union of linear subspaces is a too crude model and we need to represent nonlinear geometries like a $k$-dimensional submanifold of $\R^d$.  To deal with such complicated geometries, dimensionality reduction techniques are not sufficient. Another route is to approximate shapes by {\em simplicial complexes}~\cite{hh-ct-2010}.  Simplicial complexes have been introduced by Poincar\'e in the early days of algebraic topology. Their importance in low dimensions cannot be overestimated~: 1-dimensional simplicial complexes, better known as graphs or networks, can be found everywhere, 2-dimensional simplicial complexes are the standard representations for surfaces in computer graphics and 3-dimensional simplicial complexes are the representation of choice for scientific computing and numerical simulation when complicated domains are involved. Simplicial complexes can be defined in any dimension and can be used to construct a piecewise linear approximation of a shape with the right topological type~\cite{geometrica-7142i} or to 
compute the homology of the shape~\cite{hh-ct-2010}. The main bottleneck in using simplicial complexes in higher dimensions is of a computational nature~:  in high dimensions, simplicial complexes can be huge and very difficult to compute, which limits their current use to low dimensions.

\paragraph{Computational issues.}
The high computational cost of simplicial complexes can be balanced in two ways. First, simplicial complexes are flexible enough to approximate subspaces with a complexity that scales with the complexity of the subspace, not of the ambient space. Typically, the subspaces of interest 
 in learning theory, data analysis or dynamical systems have a moderate  intrinsic dimension.

The second reason is that coarse topological features such as the homology groups or the Betti numbers (that are invariant under homotopy equivalence) can be estimated from simplicial complexes of a much smaller size than what is needed to reconstruct a homeomorphic approximation~\cite{co-tpr-2008}. Hence, simplicial complexes may be used to recover important information even if an accurate representation of the data is out of reach.

Simplicial complexes thus have the potential of being the data structure of choice for understanding geometry in high dimensional spaces. The issue is however to define simplicial complexes that are dimension-sensitive and easy to compute.  Various types of simplicial complexes have been proposed such as the \v{C}ech and the Rips complexes, and more recent Delaunay-like complexes such as the $\alpha$-complex~\cite{eks-sspp-83,he-ubds-95}, the witness complex~\cite{deSilva2008,cds-tewc-2004} and the Delaunay tangential complex~\cite{geometrica-7142i}. They differ by their combinatorial and algorithmic complexities, and their power to approximate a shape.  The algorithmic theory of simplicial complexes is nevertheless in its infancy and much less developed than its counterpart for triangulations in small dimensions~\cite{by-ag-98,he-gtmg-2001}. In particular, little is known about efficient and compact representations of simplicial complexes and experiments have only been reported for simplicial complexes of low-dimensions up to now ~\cite{geometrica-6743i,Attali2011,rg-bptd-2008}. \framebox{true?} 


\paragraph{Geometry understanding.}

The last decade has seen tremendous progress in processing high-dimensional shapes and spaces. In {\em robotics}, randomized techniques have been proposed to construct graphs that capture the connectivity of configuration spaces and allow to search paths~\cite{sml-pa-2006}. 
In engineering and science, {\em dynamical systems} are often formulated as a function that satisfies some set of nonlinear equations, for example, the Navier-Stokes equations, Maxwell's equations, or Newton's law. Computing a  solution in the form of a mesh can be done using
higher-dimensional continuation method~\cite{mh-mpc-2002}.  However, theoretical guarantees are very few~\cite{boissonnat2010meshing} and  {\em mesh generation} in high dimensional spaces is still much less developed than its counterpart in low dimensions~\cite{he-gtmg-2001,geometrica-ecg-book}.  

In geometric {\em data analysis}, the tenet is that to effectively exploit datasets, one needs to identify, extract and analyze their underlying geometric structure.  In low dimensions, effective {\em reconstruction} techniques exist that can provide faithful approximations of the underlying structures from samples~\cite{dey-csr-2007}. Further processing makes it possible to study their topological and geometric properties. In high dimensions, however, the data often suffers from significant defects, including sparsity, noise, and outliers, violating sampling conditions required by extant methods. The problem is further compounded by the rapid growth in complexity of the data structures used for reconstruction as the dimensionality of the data increases, making them intractable in high dimensions. 


To face these challenges, researchers have proposed algorithmic tools that can infer some of the properties of the structures underlying the data without full reconstruction. Dimensionality reduction techniques mentionned above have been pretty successful in this vein % : they can infer the intrinsic dimensionality of the data, as well as provide structure-preserving mappings of the data into lower-dimensional spaces when such mappings exist. These techniques however are
but have only been proved to converge when limited to simple shapes with trivial topology.  Recently, {\em topological methods} were proposed to overcome these limitations. They led to the beautiful developments of {\em persistent homology}~\cite{eh-ph-2008} and {\em geometric inference}~\cite{geometrica-ccl09}. Although of a fundamental nature, these advances attracted interest in several fields like data analysis, computer vision or sensor networks~\cite{rg-bptd-2008}. The bottleneck that still prevents applications to benefit from the full potential of these new methods is the lack of efficient data structures and algorithms to construct simplicial complexes in high dimensions.


% \paragraph{Noisy data.} \framebox{to be revised}
% When dealing with approximation and samples, one needs stability results to ensure that the quantities that are computed, geometric or topological invariants, are good approximations of the real ones. {\em Topological persistence} was recently introduced as a powerful tool for the study of the topological invariants of sampled spaces~\cite{eh-ph-2008,rg-bptd-2008}. Given a point cloud in Euclidean space, the approach consists in building a simplicial complex whose elements are filtered by some user-defined function. This filter basically gives an order of insertion of the simplices in the complex. The persistence algorithm, first introduced by Edelsbrunner, Letscher and Zomorodian \cite{elz-tps-2002}, is able to track down the topological invariants of the filtered complex as the latter is being built. As proved by Cohen-Steiner et al. \cite{geometrica-cseh-07}, under reasonable conditions on the input point cloud, and modulo a right choice of filter, the most persistent invariants in the filtration correspond to invariants of the space underlying the data. Thus, the information extracted by the persistence algorithm is global, as opposed to the locality relationships used by the dimensionality reduction techniques. In this respect, topological persistence appears as a complementary tool to dimensionality reduction. In particular, it enables to determine whether the input data is sampled from a manifold with trivial topology, a mandatory condition for dimensionality reduction to work properly. Note however that it does not tell how and where to cut the data to remove unwanted topological features.

% Multiscale reconstruction is a novel approach~\cite{geometrica-bgo-09}. Taking advantage of the ideas of persistence, the approach consists in building a one-parameter family of simplicial complexes approximating the input at various scales. Differently from above, the family may not necessarily form a filtration, but it has other nice properties. In particular, for a sufficiently dense input data set, the family contains a long sequence of complexes that approximate the underlying space provably well, both in a topological and in a geometric sense. In fact, there can be several such sequences, each one corresponding to a plausible reconstruction at a certain scale. Thus, determining the topology and shape of the original object reduces to finding the stable sequences in the one-parameter family of complexes.   However, multiscale reconstruction, at least in its current form, still has a complexity that scales up exponentially with the dimension of the ambient space. Hence, it can only be applied to low-dimensional data sets in practice.


\paragraph{The  virtuous circle  of research and development in computational geometry.}
Experimental research and implementation are by now major components of research in computational geometry.  
Combining in a closer way basic theoretical research and the development of robust software
has been very successful in shaping the field and promoting computational geometry tools outside the community.
% is the study of effective methods (algorithms and data structures) of geometric computing, namely methods that are not only theoretically proved but also work well in practice.  
% Several EC projects (CGAL, GALIA) established an outstanding research momentum and gave a leading role to Europe in this context.  They led to successful techniques and tools, most notably 
One of the major achievements of the field has been the CGAL library that provides a well-organised, robust and efficient software environment for developing geometric applications~\cite{cgal}. CGAL is by now the standard in geometric computing, with a large diffusion worldwide and varied applications in both academia and industry.  My research group at INRIA took a leading role in the development of CGAL since its start more than 10 years ago. More recently, we seconded our basic research on 3D shape processing by practical developments in the form of fast, safe and quality-guaranteed software components for mesh generation and shape reconstruction.  Those components  are now part of the open source library CGAL  and used worldwide in academia and in industry for various applications in geometric modeling, medical imaging and geology ~\cite{cgal:rty-m3-11}.. The functionalities of CGAL in higher dimensions are quite limited. Besides, only very few prototype software have been developed for geometry understanding in higher dimensions (see http://comptop.stanford.edu/programs/).

\paragraph{Objectives.} In view of the state-of-the-art, our main objective is 
to develop an effective theory of geometry understanding in higher dimensions. We aim at processing general geometries represented as simplicial complexes. The computational bottleneck will be bypassed by assuming that the objects of interest are of moderate intrinsic dimension possibly embedded in high dimensional spaces. We intend to develop practical algorithms to mesh or reconstruct highly nonlinear manifolds, and to infer geometric and topological properties from data under realistic conditions. A major outcome of the project will be a high-quality open source library of components implementing the main results.

% Processing and analyzing complex 3D shapes is a fundamental problem with a long history of scientific successes which is on the agenda of several communities like scientific computing, computer graphics, geometry processing and computer-aided design.  Emblematic problems are mesh generation that aims at sampling and meshing a given domain, and surface reconstruction that constructs an approximation of a surface which is only known through a set of points. During the last decade, the computational geometry community established solid theoretical foundations to these problems leading to recent breakthroughs in {\em mesh generation} \cite{geometrica-bcmrv-ms-06} and {\em surface reconstruction} \cite{dey-csr-2007}.  The Geometrica group took a leading role in this research and contributed major theoretical advances as well as practical developments in the form of fast, safe and quality-guaranteed software components for mesh generation and shape reconstruction that are now part of the open source library CGAL~\cite{cgal:rty-m3-11}. Those components are now used worldwide in academia and in industry for various applications in Geometric Modeling, Medical Imaging and Geology.




\subsection{Methodology}
To reach our objectives, we will follow the principles that have been guiding my group for more than 10 years and will simultaneously develop
{\em mathematical approaches} providing guarantees even in the presence of noise or outliers,
{\em effective algorithms} that are amenable to theoretical analysis and fully validated experimentally,
and {\em perennial software} development.

The proposal is structured into the following four workpackages:
{\bf WP 1}:  {\em Dimension-sensitive data  structures} will extend current knowledge about simplicial complexes, and  provide efficient data structures and basic algorithms for their representation, construction and manipulation. 
  {\bf WP 2}:  {\em Triangulating non Euclidean geometric spaces} will develop effective algorithms to mesh or reconstruct manifolds and other spaces equipped with various metrics.   {\bf WP 3}: {\em Robust models for geometric inference, comparison and  clustering} will provide the crucial  algorithms for topological data analysis.
 {\bf WP 4}:  {\em  Software platform for geometric understanding in high dimensions} will provide the software environment for experimenting with our new data structures and algorithms, for integrating them in a coherent library of interoperable modules, and for diffusing our results to applied fields. We now describe in more detail each of these workpackages.


\input{wp1}
\input{wp2}
\input{wp3}
\input{wp4}


\bibliographystyle{abbrv}
\bibliography{erc}

\section{Resources}

I will devote 70\% of my time to this project, and I will dedicate all my expertise and efforts to conduct and supervise the research work. To this end, I will receive the precious help of 2 permanent researchers of the Geometrica team : Fr\'ed\'eric Chazal who is a  leading researcher in geometric inference and computational topology and Mariette Yvinec who is an expert in geometric computing and a member of the CGAL Editorial Board. They will devote 20\% of their time to this project to co-supervise with me the research and implementation work of the students, postdocs and engineers to be engaged in this project. Other members of Geometrica,
not financially supported by this project, will also collaborate to the project.

